{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short text classification bake-off!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blog posts showing off deep learning approaches for some task are now a dime a dozen. One thing I *haven't* come across too much is blog posts comparing \"standard\" approaches to text classification.\n",
    "\n",
    "In this notebook, I'm going to compare logistic regression on bag-of-ngrams features (a standard industry workhorse for text classification) with a ConvNet (the \"new kid\" baseline) on a simple Twitter sentiment analysis task.\n",
    "\n",
    "*N.B.* This work was inspired by some experiments I was doing at work on search query classification with some data that I can't share; several Twitter sent-eval datasets are freely available. It's basically structured as a semi-narrated directed exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're fetching a set of labeled Twitter sentiment analysis data from http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "url = \"http://thinknook.com/wp-content/uploads/2012/09/Sentiment-Analysis-Dataset.zip\"\n",
    "f_name = os.path.basename(url)\n",
    "r = requests.get(url, stream=True)\n",
    "with open(f_name, \"wb\") as f_out:\n",
    "    for chunk in r.iter_content(chunk_size=1024):\n",
    "        if chunk:\n",
    "            f_out.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put that in a `data` subdirectory and unzip so we can take a peek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import errno\n",
    "# check if the directory exists and handle the relevant OSError (thanks StackOverflow!)\n",
    "path = \"./data\"\n",
    "try:\n",
    "    os.makedirs(path)\n",
    "except OSError as exc:\n",
    "    if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
    "        pass\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# extract the zip\n",
    "from zipfile import ZipFile\n",
    "with ZipFile(\"Sentiment-Analysis-Dataset.zip\") as myzip:\n",
    "    myzip.extractall(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis Dataset.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's see what kind of file it is, and what it looks like..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/Sentiment Analysis Dataset.csv: UTF-8 Unicode (with BOM) text, with CRLF line terminators\r\n"
     ]
    }
   ],
   "source": [
    "!file data/Sentiment\\ Analysis\\ Dataset.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ugh, it's got a [BOM](https://en.wikipedia.org/wiki/Byte_order_mark). Why?! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿ItemID,Sentiment,SentimentSource,SentimentText\r",
      "\r\n",
      "1,0,Sentiment140,                     is so sad for my APL friend.............\r",
      "\r\n",
      "2,0,Sentiment140,                   I missed the New Moon trailer...\r",
      "\r\n",
      "3,1,Sentiment140,              omg its already 7:30 :O\r",
      "\r\n",
      "4,0,Sentiment140,          .. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2 just get a crown put on (30mins)...\r",
      "\r\n",
      "5,0,Sentiment140,         i think mi bf is cheating on me!!!       T_T\r",
      "\r\n",
      "6,0,Sentiment140,         or i just worry too much?        \r",
      "\r\n",
      "7,1,Sentiment140,       Juuuuuuuuuuuuuuuuussssst Chillin!!\r",
      "\r\n",
      "8,0,Sentiment140,       Sunny Again        Work Tomorrow  :-|       TV Tonight\r",
      "\r\n",
      "9,1,Sentiment140,      handed in my uniform today . i miss you already\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head data/Sentiment\\ Analysis\\ Dataset.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, it's basic CSV; let's try to load it up the naive way and hope that there are no commas in the final field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\xef\\xbb\\xbfItemID', 'Sentiment', 'SentimentSource', 'SentimentText']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "f_in = open(\"data/Sentiment Analysis Dataset.csv\")\n",
    "reader = csv.reader(f_in, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "header = reader.next()\n",
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1578614"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = []\n",
    "for row in reader:\n",
    "    label = int(row[1])\n",
    "    tweet = row[-1]\n",
    "    all_data.append((tweet, label))\n",
    "\n",
    "len(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, about 1.5M data points; nice. Now let's shuffle it, then split into train/val/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "shuffle(all_data)\n",
    "VAL_SPLIT = 0.1\n",
    "nb_val_samples = int(VAL_SPLIT * len(all_data))\n",
    "# doing an 80/10/10 split\n",
    "train_data = all_data[:-2*nb_val_samples]\n",
    "dev_data = all_data[-2*nb_val_samples:-nb_val_samples]\n",
    "test_data = all_data[-nb_val_samples:]\n",
    "\n",
    "len(train_data)\n",
    "len(dev_data)\n",
    "len(test_data)\n",
    "assert len(all_data) == len(train_data) + len(dev_data) + len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final peek at the raw training data to make sure it mostly looks like what we expect..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tonight was amazing, but I miss the good old days ', 0),\n",
       " (\"@volupty it's only cool because tomorrow my blond cousin comes  and his blue eyes too :p\",\n",
       "  1),\n",
       " ('@mrskutcher definatley agree  blew flawless out of the runnings with that performance!!',\n",
       "  1),\n",
       " ('@newO_nyboR yeahh haha. Hope u feel better sooon ', 1),\n",
       " ('just got off the phone with my Daddy Doodle. I miss him.  http://plurk.com/p/114ons',\n",
       "  0)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import sample\n",
    "sample(train_data, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*N.B.* Given the approach I'm going to take below; we're going to replace all @-mentions with a fixed string (e.g. `\"AT_MENTION\"`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by separating inputs and labels, then do the @-mention replacements\n",
    "train_inputs, train_labels = zip(*train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "X_train = []\n",
    "# this is *very* coarse and will surely match things that we don't want it to\n",
    "mention_patt = re.compile(ur'@\\w+', re.UNICODE)\n",
    "for item in train_inputs:\n",
    "    try:\n",
    "        X_train.append(mention_patt.sub(\"AT_MENTION\", item.decode(\"utf8\")))\n",
    "    except Exception:\n",
    "        print item\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, looks like that worked. Onward!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First contender: old-school LR+BoW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Insert clever/insightful stuff about LR + BoW approaches here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the model\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "# using 2 cores (of my 4) for fitting\n",
    "lr = LogisticRegressionCV(n_jobs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*N.B.* We're using `LogisticRegressionCV` rather than plain `LogisticRegression` because the former does cross-validation on the `C` regularizer for free as part of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the feature extractor\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer(analyzer=\"char\", lowercase=False, binary=False, ngram_range=(3, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `\"char\"` analyzer extracts character-based ngrams, *including across word boundaries*. I'm keeping casing AS-IS because *a priori* I can imagine it being helpful in deciding whether some piece of text has emotional weight. The range of ngram sizes I'm looking at is not justified beyond it having worked for me in the past in other text classification scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 37s, sys: 8.83 s, total: 3min 46s\n",
      "Wall time: 19min 31s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vec', CountVectorizer(analyzer='char', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=False, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(3, 6), preprocessor=None, stop_words=None,\n",
       "        str...2', random_state=None,\n",
       "           refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put them together in an end-to-end trainable/callable pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline([(\"vec\", vec), (\"lr\", lr)])\n",
    "\n",
    "# let's fit it on 100k data points and see how long it takes\n",
    "%time pipeline.fit(X_train[:100000], train_labels[:100000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20 minutes...so we can ballpark ~4hrs for the full training set. I've seen reports that DL classification really only comes into its own (metrics-wise) with datasets of size *O(1e6)*...let's see what we can get out of this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second contender: new-school ConvNet/CNN\n",
    "### (where \"new\" == \"already outdated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach we're taking here is a fairly standard approach to using ConvNets for sentence classification, without much in the way of bells, or whistles. [Kim (2014)](https://arxiv.org/abs/1408.5882) is one of the canonical references for this approach. Our architecture closely resembles the one used there, and pictured here (source: Denny Britz's [blog post on CNNs for NLP](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/); one of the clearest expositions of this I've come across):\n",
    "\n",
    "<img src=\"http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/11/Screen-Shot-2015-11-06-at-12.05.40-PM.png\" alt=\"ConvNet for sentence classification\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**N.B.** The principle difference in the approach taken here is that I'll be using *character*-based embeddings (trained from scratch), rather than word embeddings, as shown above (and used in Kim 2014). Mostly this is because I used char ngrams for the BoNG+LR classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# lots of stuff to import\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Activation, concatenate, Embedding, Conv1D, GlobalMaxPooling1D\n",
    "from keras.utils.np_utils import *\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import *\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**N.B.** I'm using the _CPU_ for this. My Mac has a GeForce GT 650M, but I recently upgrade CUDA and drivers, and things are a bit borked (w.r.t. getting Keras to see the GPU).\n",
    "\n",
    "We'll start with defining some useful bits of stuff here, for easier tweaking later on (should it prove necessary), and then do some data munging to get it into the shape that our model needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing\n",
      "Found 185 unique tokens\n",
      "Padding, encoding, train/dev split\n"
     ]
    }
   ],
   "source": [
    "# DEFINE SOME USEFUL STUFF\n",
    "# some of these vals are cribbed from other experiments with\n",
    "# other short text classification tasks; YMMV, caveat emptor, yadda yadda...do some experimenting\n",
    "VALIDATION_SPLIT = 0.1\n",
    "EMBEDDING_DIM = 32\n",
    "# max len in first 100k is 317, but very few longer than 256\n",
    "MAX_SEQUENCE_LENGTH = 256\n",
    "BATCH_SIZE = 64\n",
    "# keeping this small(ish) to keep training time down\n",
    "FILTERS = 50\n",
    "HIDDEN_DIMS = 250\n",
    "P_DROPOUT = 0.25\n",
    "EPOCHS = 2\n",
    "\n",
    "\n",
    "# using the tokenizer that comes with Keras\n",
    "# this fxn also maps the input vocab to sequences of integer indices and returns a char-to-index map\n",
    "def tokenize_texts(texts):\n",
    "    print('Tokenizing')\n",
    "    tokenizer = Tokenizer(char_level=True)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %d unique tokens' % (len(word_index),))\n",
    "    return sequences, word_index\n",
    "\n",
    "\n",
    "# truncate/pad input sequences so that they're all the same length\n",
    "def pad_seqs(sequences):\n",
    "    print('Padding, encoding, train/dev split')\n",
    "    data = pad_sequences(sequences,\n",
    "                         maxlen=MAX_SEQUENCE_LENGTH,\n",
    "                         padding='post',\n",
    "                         truncating='post')\n",
    "    return data\n",
    "\n",
    "\n",
    "sequences, word_index = tokenize_texts(X_train[:100000])\n",
    "data = pad_seqs(sequences)\n",
    "import numpy\n",
    "labels = numpy.asarray(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll be using Keras's [functional API](https://keras.io/getting-started/functional-api-guide/). I've only started using it, and already prefer it (plus it's a bit closer to the [PyTorch](http://pytorch.org) approach, which I've finally started playing with). OK, let's define the main building blocks of the model (again, there are design decisions in here that come from previous experimentation, literature-based suggestions, and Twitter-suggested rules of thumb; normally you'd want to use some kind of hyperparam optimization for this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "# zeroth index doesn't get used by Keras in embedding layers\n",
    "emb = Embedding(len(word_index) + 1,\n",
    "                EMBEDDING_DIM,\n",
    "                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                trainable=True)(x)\n",
    "# we'll set up convolutions of size 3, 4, 5, and 6, like the ngram_range we used above,\n",
    "# do max-pooling on each, then concatenate the output before running it through a dense layer\n",
    "conv_3 = Conv1D(FILTERS, 3, padding=\"valid\",\n",
    "                activation=\"relu\", strides=1)(emb)\n",
    "maxpool_3 = GlobalMaxPooling1D()(conv_3)\n",
    "conv_4 = Conv1D(FILTERS, 4, padding=\"valid\",\n",
    "                activation=\"relu\", strides=1)(emb)\n",
    "maxpool_4 = GlobalMaxPooling1D()(conv_4)\n",
    "conv_5 = Conv1D(FILTERS, 5, padding=\"valid\",\n",
    "                activation=\"relu\", strides=1)(emb)\n",
    "maxpool_5 = GlobalMaxPooling1D()(conv_5)\n",
    "conv_6 = Conv1D(FILTERS, 6, padding=\"valid\",\n",
    "                activation=\"relu\", strides=1)(emb)\n",
    "maxpool_6 = GlobalMaxPooling1D()(conv_6)\n",
    "merged = concatenate([maxpool_3, maxpool_4, maxpool_5, maxpool_6], axis=-1)\n",
    "hidden = Dense(HIDDEN_DIMS)(merged)\n",
    "dropout = Dropout(P_DROPOUT)(hidden)\n",
    "dropout = Activation(\"relu\")(dropout)\n",
    "out = Dense(1, activation=\"sigmoid\")(dropout)\n",
    "model = Model(inputs=x, outputs=out)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright; model defined and data ready. Let's see how long it takes to train this bad boy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 268s - loss: 0.5681 - acc: 0.6989   \n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 275s - loss: 0.4964 - acc: 0.7583   \n",
      "CPU times: user 35min 18s, sys: 46 s, total: 36min 4s\n",
      "Wall time: 9min 9s\n"
     ]
    }
   ],
   "source": [
    "%time hist = model.fit(data[:100000], labels[:100000], batch_size=BATCH_SIZE, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Half the time to train! Can't say I expected that...now let's see whether these classifiers are any good at their assigned tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of my pet peeves about posts like this is the lack of detail w.r.t. things like system hardware/architecture,\n",
    "training time, etc. as well as the simplicity of the eval metrics (usually just raw accuracy).\n",
    "\n",
    "*Blah blah, more to say here about that...*\n",
    "\n",
    "Need to munge the dev data the same way we did the training data, for both the LR and CNN use cases (remove @-mentions, turn into ndarrays, sequencify, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_inputs, dev_labels = zip(*dev_data)\n",
    "\n",
    "X_dev = []\n",
    "for item in dev_inputs:\n",
    "    try:\n",
    "        X_dev.append(mention_patt.sub(\"AT_MENTION\", item.decode(\"utf8\")))\n",
    "    except Exception:\n",
    "        print item\n",
    "        break\n",
    "\n",
    "y_dev = np.asarray(dev_labels[:10000])\n",
    "y_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.801     0.790     0.796      5010\n",
      "          1      0.792     0.803     0.798      4990\n",
      "\n",
      "avg / total      0.797     0.797     0.797     10000\n",
      "\n",
      "CPU times: user 3.07 s, sys: 61.4 ms, total: 3.13 s\n",
      "Wall time: 3.13 s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "%time print classification_report(y_dev[:10000], pipeline.predict(X_dev[:10000]), digits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing\n",
      "Found 168 unique tokens\n",
      "Padding, encoding, train/dev split\n"
     ]
    }
   ],
   "source": [
    "dev_sequences, _ = tokenize_texts(X_dev[:10000])\n",
    "X_dev_cnn = pad_seqs(dev_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.785     0.711     0.746      5010\n",
      "          1      0.735     0.804     0.768      4990\n",
      "\n",
      "avg / total      0.760     0.757     0.757     10000\n",
      "\n",
      "CPU times: user 33.5 s, sys: 327 ms, total: 33.8 s\n",
      "Wall time: 8.5 s\n"
     ]
    }
   ],
   "source": [
    "%time print classification_report(y_dev[:10000], np.round(model.predict(X_dev_cnn[:10000])).astype(\"int32\"), digits=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm...four points lower on f-score, AND almost 3 times slower in prediction. We'll need something better than this to take to the boss if we want to convince him to let us play with DL stuff.\n",
    "Let's try training for twice as long as see what comes of that. If that doesn't help, we can always try with more data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "100000/100000 [==============================] - 273s - loss: 0.4720 - acc: 0.7756   \n",
      "Epoch 2/4\n",
      "100000/100000 [==============================] - 276s - loss: 0.4535 - acc: 0.7853   \n",
      "Epoch 3/4\n",
      "100000/100000 [==============================] - 277s - loss: 0.4414 - acc: 0.7943   \n",
      "Epoch 4/4\n",
      "100000/100000 [==============================] - 279s - loss: 0.4288 - acc: 0.8011   \n",
      "CPU times: user 1h 11min 44s, sys: 1min 30s, total: 1h 13min 14s\n",
      "Wall time: 18min 26s\n"
     ]
    }
   ],
   "source": [
    "%time hist = model.fit(data[:100000], labels[:100000], batch_size=BATCH_SIZE, epochs=2*EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, we're basically at the same training time as for the LR pipeline now. The loss kept dropping, which is good, **BUT** do note that I'm checking against validation loss here; `model.fit()` lets you pass in a validation set as an additional param, and you can set up checks against the loss on the validation set---which doesn't get used as training data---to decide when to stop training. Normally I'd have done that.\n",
    "\n",
    "Alright, let's see how good this baby is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.719     0.864     0.785      5010\n",
      "          1      0.829     0.661     0.736      4990\n",
      "\n",
      "avg / total      0.774     0.763     0.760     10000\n",
      "\n",
      "CPU times: user 35.7 s, sys: 1 s, total: 36.7 s\n",
      "Wall time: 9.44 s\n"
     ]
    }
   ],
   "source": [
    "%time print classification_report(y_dev[:10000], np.round(model.predict(X_dev_cnn[:10000])).astype(\"int32\"), digits=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, not promising. We squeezed a tiny bit more performance out, in terms of average metrics, but we're still a few points away from the scores the basic system got.\n",
    "\n",
    "Let's try training on twice as much data...don't forget that we truncated `data` right when we defined it up above, so let's revisit that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing\n",
      "Found 196 unique tokens\n",
      "Padding, encoding, train/dev split\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(500000, 256)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences, word_index = tokenize_texts(X_train[:500000])\n",
    "data = pad_seqs(sequences)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      " 13312/200000 [>.............................] - ETA: 507s - loss: 0.4519 - acc: 0.7866"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 189 is out of bounds for size 186\nApply node that caused the error: AdvancedSubtensor1(embedding_2/embeddings, Reshape{1}.0)\nToposort index: 48\nInputs types: [TensorType(float32, matrix), TensorType(int32, vector)]\nInputs shapes: [(186, 32), (16384,)]\nInputs strides: [(128, 4), (4,)]\nInputs values: ['not shown', 'not shown']\nOutputs clients: [[Reshape{3}(AdvancedSubtensor1.0, MakeVector{dtype='int64'}.0)]]\n\nBacktrace when the node is created(use Theano flag traceback.limit=N to make it longer):\n  File \"/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-114-3a8528a90ed6>\", line 6, in <module>\n    trainable=True)(x)\n  File \"/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/keras/engine/topology.py\", line 602, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/keras/layers/embeddings.py\", line 134, in call\n    out = K.gather(self.embeddings, inputs)\n  File \"/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/keras/backend/theano_backend.py\", line 483, in gather\n    y = reference[indices]\n\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-161-d40484337dfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time hist = model.fit(data[:200000], labels[:200000], batch_size=BATCH_SIZE, epochs=2*EPOCHS)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2157\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2158\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2077\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2078\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2079\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2080\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m             \u001b[0;32mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    896\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[1;32m    899\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m                 \u001b[0;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/theano/gof/link.pyc\u001b[0m in \u001b[0;36mraise_with_op\u001b[0;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# extra long error message in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 189 is out of bounds for size 186\nApply node that caused the error: AdvancedSubtensor1(embedding_2/embeddings, Reshape{1}.0)\nToposort index: 48\nInputs types: [TensorType(float32, matrix), TensorType(int32, vector)]\nInputs shapes: [(186, 32), (16384,)]\nInputs strides: [(128, 4), (4,)]\nInputs values: ['not shown', 'not shown']\nOutputs clients: [[Reshape{3}(AdvancedSubtensor1.0, MakeVector{dtype='int64'}.0)]]\n\nBacktrace when the node is created(use Theano flag traceback.limit=N to make it longer):\n  File \"/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-114-3a8528a90ed6>\", line 6, in <module>\n    trainable=True)(x)\n  File \"/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/keras/engine/topology.py\", line 602, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/keras/layers/embeddings.py\", line 134, in call\n    out = K.gather(self.embeddings, inputs)\n  File \"/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/keras/backend/theano_backend.py\", line 483, in gather\n    y = reference[indices]\n\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "%time hist = model.fit(data[:200000], labels[:200000], batch_size=BATCH_SIZE, epochs=2*EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
