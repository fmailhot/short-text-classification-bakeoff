{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short text classification bake-off!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blog posts showing off deep learning approaches for some task are now a dime a dozen. One thing I *haven't* come across too much is blog posts comparing \"standard\" approaches to text classification.\n",
    "\n",
    "In this notebook, I'm going to compare logistic regression on bag-of-ngrams features (a standard industry workhorse for text classification) with a ConvNet (the \"new kid\" baseline) on a simple Twitter sentiment analysis task.\n",
    "\n",
    "*N.B.* This work was inspired by some experiments I was doing at work on search query classification with some data that I can't share; several Twitter sent-eval datasets are freely available. It's basically structured as a semi-narrated directed exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Javascript \"beep\" test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "// Source - https://stackoverflow.com/a/23395136\n",
       "\n",
       "Jupyter.beep = () => {\n",
       "    var snd = new Audio(\"data:audio/wav;base64,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\");  \n",
       "    snd.play();\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "// Source - https://stackoverflow.com/a/23395136\n",
    "\n",
    "Jupyter.beep = () => {\n",
    "    var snd = new Audio(\"data:audio/wav;base64,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\");  \n",
    "    snd.play();\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "Jupyter.beep()"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "Jupyter.beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COOL, THAT WORKED.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're fetching a set of labeled Twitter sentiment analysis data from http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "url = \"http://thinknook.com/wp-content/uploads/2012/09/Sentiment-Analysis-Dataset.zip\"\n",
    "f_name = os.path.basename(url)\n",
    "r = requests.get(url, stream=True)\n",
    "with open(f_name, \"wb\") as f_out:\n",
    "    for chunk in r.iter_content(chunk_size=1024):\n",
    "        if chunk:\n",
    "            f_out.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put that in a `data` subdirectory and unzip so we can take a peek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import errno\n",
    "# check if the directory exists and handle the relevant OSError (thanks StackOverflow!)\n",
    "path = \"./data\"\n",
    "try:\n",
    "    os.makedirs(path)\n",
    "except OSError as exc:\n",
    "    if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
    "        pass\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# extract the zip\n",
    "from zipfile import ZipFile\n",
    "with ZipFile(\"Sentiment-Analysis-Dataset.zip\") as myzip:\n",
    "    myzip.extractall(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis Dataset.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's see what kind of file it is, and what it looks like..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/Sentiment Analysis Dataset.csv: UTF-8 Unicode (with BOM) text, with CRLF line terminators\r\n"
     ]
    }
   ],
   "source": [
    "!file data/Sentiment\\ Analysis\\ Dataset.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ugh, it's got a [BOM](https://en.wikipedia.org/wiki/Byte_order_mark). Why?! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿ItemID,Sentiment,SentimentSource,SentimentText\r",
      "\r\n",
      "1,0,Sentiment140,                     is so sad for my APL friend.............\r",
      "\r\n",
      "2,0,Sentiment140,                   I missed the New Moon trailer...\r",
      "\r\n",
      "3,1,Sentiment140,              omg its already 7:30 :O\r",
      "\r\n",
      "4,0,Sentiment140,          .. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2 just get a crown put on (30mins)...\r",
      "\r\n",
      "5,0,Sentiment140,         i think mi bf is cheating on me!!!       T_T\r",
      "\r\n",
      "6,0,Sentiment140,         or i just worry too much?        \r",
      "\r\n",
      "7,1,Sentiment140,       Juuuuuuuuuuuuuuuuussssst Chillin!!\r",
      "\r\n",
      "8,0,Sentiment140,       Sunny Again        Work Tomorrow  :-|       TV Tonight\r",
      "\r\n",
      "9,1,Sentiment140,      handed in my uniform today . i miss you already\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head data/Sentiment\\ Analysis\\ Dataset.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, it's basic CSV; let's try to load it up the naive way and hope that there are no commas in the final field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\xef\\xbb\\xbfItemID', 'Sentiment', 'SentimentSource', 'SentimentText']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "f_in = open(\"data/Sentiment Analysis Dataset.csv\")\n",
    "reader = csv.reader(f_in, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "header = reader.next()\n",
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1578614"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = []\n",
    "for row in reader:\n",
    "    label = int(row[1])\n",
    "    tweet = row[-1]\n",
    "    all_data.append((tweet, label))\n",
    "\n",
    "len(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, about 1.5M data points; nice. Now let's shuffle it, then split into train/val/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "shuffle(all_data)\n",
    "VAL_SPLIT = 0.1\n",
    "nb_val_samples = int(VAL_SPLIT * len(all_data))\n",
    "# doing an 80/10/10 split\n",
    "train_data = all_data[:-2*nb_val_samples]\n",
    "dev_data = all_data[-2*nb_val_samples:-nb_val_samples]\n",
    "test_data = all_data[-nb_val_samples:]\n",
    "\n",
    "len(train_data)\n",
    "len(dev_data)\n",
    "len(test_data)\n",
    "assert len(all_data) == len(train_data) + len(dev_data) + len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final peek at the raw training data to make sure it mostly looks like what we expect..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tonight was amazing, but I miss the good old days ', 0),\n",
       " (\"@volupty it's only cool because tomorrow my blond cousin comes  and his blue eyes too :p\",\n",
       "  1),\n",
       " ('@mrskutcher definatley agree  blew flawless out of the runnings with that performance!!',\n",
       "  1),\n",
       " ('@newO_nyboR yeahh haha. Hope u feel better sooon ', 1),\n",
       " ('just got off the phone with my Daddy Doodle. I miss him.  http://plurk.com/p/114ons',\n",
       "  0)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import sample\n",
    "sample(train_data, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*N.B.* Given the approach I'm going to take below; we're going to replace all @-mentions with a fixed string (e.g. `\"AT_MENTION\"`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start by separating inputs and labels, then do the @-mention replacements\n",
    "train_inputs, train_labels = zip(*train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "X_train = []\n",
    "# this is *very* coarse and will surely match things that we don't want it to\n",
    "mention_patt = re.compile(ur'@\\w+', re.UNICODE)\n",
    "for item in train_inputs:\n",
    "    try:\n",
    "        X_train.append(mention_patt.sub(\"AT_MENTION\", item.decode(\"utf8\")))\n",
    "    except Exception:\n",
    "        print item\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, looks like that worked. Onward!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First contender: old-school LR+BoW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Insert clever/insightful stuff about LR + BoW approaches here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the model\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "# using 2 cores (of my 4) for fitting\n",
    "lr = LogisticRegressionCV(n_jobs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*N.B.* We're using `LogisticRegressionCV` rather than plain `LogisticRegression` because the former does cross-validation on the `C` regularizer for free as part of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the feature extractor\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer(analyzer=\"char\", lowercase=False, binary=False, ngram_range=(3, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `\"char\"` analyzer extracts character-based ngrams, *including across word boundaries*. I'm keeping casing AS-IS because *a priori* I can imagine it being helpful in deciding whether some piece of text has emotional weight. The range of ngram sizes I'm looking at is not justified beyond it having worked for me in the past in other text classification scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 37s, sys: 8.83 s, total: 3min 46s\n",
      "Wall time: 19min 31s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vec', CountVectorizer(analyzer='char', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=False, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(3, 6), preprocessor=None, stop_words=None,\n",
       "        str...2', random_state=None,\n",
       "           refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put them together in an end-to-end trainable/callable pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline([(\"vec\", vec), (\"lr\", lr)])\n",
    "\n",
    "# let's fit it on 100k data points and see how long it takes\n",
    "%time pipeline.fit(X_train[:100000], train_labels[:100000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20 minutes...so we can ballpark ~4hrs for the full training set. I've seen reports that DL classification really only comes into its own (metrics-wise) with datasets of size *O(1e6)*...let's see what we can get out of this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second contender: new-school ConvNet/CNN\n",
    "### (where \"new\" == \"already somewhat outdated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach we're taking here is a fairly standard approach to using ConvNets for sentence classification, without much in the way of bells, or whistles. [Kim (2014)](https://arxiv.org/abs/1408.5882) is one of the canonical references for this approach. Our architecture closely resembles the one used there, and pictured here (source: Denny Britz's [blog post on CNNs for NLP](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/); one of the clearest expositions of this I've come across):\n",
    "\n",
    "<img src=\"http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/11/Screen-Shot-2015-11-06-at-12.05.40-PM.png\" alt=\"ConvNet for sentence classification\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**N.B.** The principle difference in the approach taken here is that I'll be using *character*-based embeddings (trained from scratch), rather than word embeddings, as shown above (and used in Kim 2014). Mostly this is because I used char ngrams for the BoNG+LR classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# lots of stuff to import\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Activation, concatenate, Embedding, Conv1D, GlobalMaxPooling1D\n",
    "from keras.utils.np_utils import *\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import *\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**N.B.** I'm using the _CPU_ for this. My Mac has a GeForce GT 650M, but I recently upgrade CUDA and drivers, and things are a bit borked (w.r.t. getting Keras to see the GPU).\n",
    "\n",
    "We'll start with defining some useful bits of stuff here, for easier tweaking later on (should it prove necessary), and then do some data munging to get it into the shape that our model needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing\n",
      "Found 185 unique tokens\n",
      "Padding, encoding, train/dev split\n"
     ]
    }
   ],
   "source": [
    "# DEFINE SOME USEFUL STUFF\n",
    "# some of these vals are cribbed from other experiments with\n",
    "# other short text classification tasks; YMMV, caveat emptor, yadda yadda...do some experimenting\n",
    "VALIDATION_SPLIT = 0.1\n",
    "EMBEDDING_DIM = 32\n",
    "# max len in first 100k is 317, but very few longer than 256\n",
    "MAX_SEQUENCE_LENGTH = 256\n",
    "BATCH_SIZE = 64\n",
    "# keeping this small(ish) to keep training time down\n",
    "FILTERS = 50\n",
    "HIDDEN_DIMS = 250\n",
    "P_DROPOUT = 0.25\n",
    "EPOCHS = 2\n",
    "\n",
    "\n",
    "# using the tokenizer that comes with Keras\n",
    "# this fxn also maps the input vocab to sequences of integer indices and returns a char-to-index map\n",
    "def tokenize_texts(texts):\n",
    "    print('Tokenizing')\n",
    "    tokenizer = Tokenizer(char_level=True)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %d unique tokens' % (len(word_index),))\n",
    "    return sequences, word_index\n",
    "\n",
    "\n",
    "# truncate/pad input sequences so that they're all the same length\n",
    "def pad_seqs(sequences):\n",
    "    print('Padding, encoding, train/dev split')\n",
    "    data = pad_sequences(sequences,\n",
    "                         maxlen=MAX_SEQUENCE_LENGTH,\n",
    "                         padding='post',\n",
    "                         truncating='post')\n",
    "    return data\n",
    "\n",
    "\n",
    "sequences, word_index = tokenize_texts(X_train[:100000])\n",
    "data = pad_seqs(sequences)\n",
    "import numpy\n",
    "labels = numpy.asarray(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll be using Keras's [functional API](https://keras.io/getting-started/functional-api-guide/). I've only started using it, and already prefer it (plus it's a bit closer to the [PyTorch](http://pytorch.org) approach, which I've finally started playing with). OK, let's define the main building blocks of the model (again, there are design decisions in here that come from previous experimentation, literature-based suggestions, and Twitter-suggested rules of thumb; normally you'd want to use some kind of hyperparam optimization for this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "# zeroth index doesn't get used by Keras in embedding layers\n",
    "emb = Embedding(len(word_index) + 1,\n",
    "                EMBEDDING_DIM,\n",
    "                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                trainable=True)(x)\n",
    "# we'll set up convolutions of size 3, 4, 5, and 6, like the ngram_range we used above,\n",
    "# do max-pooling on each, then concatenate the output before running it through a dense layer\n",
    "conv_3 = Conv1D(FILTERS, 3, padding=\"valid\",\n",
    "                activation=\"relu\", strides=1)(emb)\n",
    "maxpool_3 = GlobalMaxPooling1D()(conv_3)\n",
    "conv_4 = Conv1D(FILTERS, 4, padding=\"valid\",\n",
    "                activation=\"relu\", strides=1)(emb)\n",
    "maxpool_4 = GlobalMaxPooling1D()(conv_4)\n",
    "conv_5 = Conv1D(FILTERS, 5, padding=\"valid\",\n",
    "                activation=\"relu\", strides=1)(emb)\n",
    "maxpool_5 = GlobalMaxPooling1D()(conv_5)\n",
    "conv_6 = Conv1D(FILTERS, 6, padding=\"valid\",\n",
    "                activation=\"relu\", strides=1)(emb)\n",
    "maxpool_6 = GlobalMaxPooling1D()(conv_6)\n",
    "merged = concatenate([maxpool_3, maxpool_4, maxpool_5, maxpool_6], axis=-1)\n",
    "hidden = Dense(HIDDEN_DIMS)(merged)\n",
    "dropout = Dropout(P_DROPOUT)(hidden)\n",
    "dropout = Activation(\"relu\")(dropout)\n",
    "out = Dense(1, activation=\"sigmoid\")(dropout)\n",
    "model = Model(inputs=x, outputs=out)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright; model defined and data ready. Let's see how long it takes to train this bad boy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 268s - loss: 0.5681 - acc: 0.6989   \n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 275s - loss: 0.4964 - acc: 0.7583   \n",
      "CPU times: user 35min 18s, sys: 46 s, total: 36min 4s\n",
      "Wall time: 9min 9s\n"
     ]
    }
   ],
   "source": [
    "%time hist = model.fit(data[:100000], labels[:100000], batch_size=BATCH_SIZE, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Half the time to train! Can't say I expected that...now let's see whether these classifiers are any good at their assigned tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third contender: old-school-new-kid (py)FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[FastText](https://github.com/facebookresearch/fastText) is a library released by the **Facebook AI Research** (FAIR) group in 2016. It's very specifically designed for the efficient training of high-quality word embeddings and text classifiers, and by all accounts is at or near state-of-the-art on these tasks, and should at the very least be considered a strong baseline to beat for any new models.\n",
    "\n",
    "We'll be using the [pyfasttest](https://github.com/vrasneur/pyfasttext) Python bindings here.\n",
    "(*N.B.* Installing via `pip` resulted in some issues for me (on Mac Sierra). I had to `conda install gcc`, then use that compiler to run `python setup.py install`, and now everything seems to be peachy.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyfasttext import FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastText requires training data to be in files, and in a specific format, so we'll handle that here with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "with codecs.open(\"data/sentiment.train.ft.txt\", \"w\") as f_out:\n",
    "    for label, datum in zip(train_labels[:100000], X_train[:100000]):\n",
    "        try:\n",
    "            f_out.write(\"%s %s\" % (\"__label__good\" if int(label) else \"__label__bad\", datum.encode(\"utf8\")) + \"\\n\")\n",
    "        except UnicodeEncodeError:\n",
    "            print datum\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek to make sure that looks right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__bad AT_MENTION i feel for you, ? ??? ???????? full ???? ?? ???????????????  it hits the theaters in 9 days tho! i m waiting impatiently!!\r\n",
      "__label__good hi iÂ´m from german pleas helf \r\n",
      "__label__good AT_MENTION did I send you wrong state...no he was in MN-Minnesota...my mind is blogged \r\n",
      "__label__good AT_MENTION Hi, Yesterday I finaly mixed with Ina the purple nurple. \r\n",
      "__label__bad Insomnia. D'you know... It's 2am and I still can't keep my eyes closing. Damn! Again? I bet it's 4 or 5 \r\n",
      "__label__good ohhh degrassi marathon marryy me  ha playing cards with my sister &amp;; watching degrassi, it's the one where rickk shoots jimmy ://\r\n",
      "__label__bad AT_MENTION Haha Ernest! ....buries his mother? You lie! Rest in Peace Jim Varney \r\n",
      "__label__bad waiting for everyone to leave so i can have the house to myself. working tonight \r\n",
      "__label__bad AT_MENTION the clash are too good for their armand van halen ears. \r\n",
      "__label__good AT_MENTION  U leave tomorrow?  I'm so excited for you!!  Have a safe trip   Also, can you smuggle an Alpaca for me?  I've always wanted one.\r\n"
     ]
    }
   ],
   "source": [
    "!head data/sentiment.train.ft.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems OK enough for this. Training is nice and simple, and according to the folks at FAIR, it's lightning fast (FastText is chock full of optimizations that make it quick to train, quick to use, AND space efficient)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.5 s, sys: 1.95 s, total: 24.4 s\n",
      "Wall time: 15.1 s\n"
     ]
    }
   ],
   "source": [
    "ft = FastText()\n",
    "# we'll do 2 epochs, like we did (to start) for the ConvNet\n",
    "# \n",
    "# I the options on the FastText website and am using values to mimic o|ur previous examples\n",
    "%time ft.supervised(input=\"data/sentiment.train.ft.txt\", output=\"sentiment_model\", verbose=3, epoch=EPOCHS, lr=0.1, minn=3, maxn=6, wordNgrams=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holy cripes **15s**! That's nearly two orders of magnitude faster than our previous models. That gives A LOT more time for experimentation and hyperparam tuning.\n",
    "\n",
    "Are we sure we trained on the same amount of data?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100001 data/sentiment.train.ft.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l data/sentiment.train.ft.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough.\n",
    "\n",
    "Alright, time to get down to brass tacks and see how these things actually fare, metrics-wise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of my pet peeves about posts like this is the lack of detail w.r.t. things like system hardware/architecture,\n",
    "training time, etc. as well as the simplicity of the eval metrics (usually just raw accuracy).\n",
    "\n",
    "In the spirit of transparency and (one hopes) replicability, here's what I'm working with.\n",
    "\n",
    "### Specs\n",
    "\n",
    "I'm on a MacBook Pro with\n",
    "\n",
    "* 16GB 1600 MHz DDR3 RAM\n",
    "* 2.6 GHz Intel Core i7 processor\n",
    "* NVIDIA GeForce 650M GPU (not that I'm using that thus far)\n",
    "* running macos Sierra.\n",
    "\n",
    "### Metrics\n",
    "\n",
    "Rather than just look at raw accurancy (which is distressingly frequent in DL papers I've come across), we're going to be looking at precision and recall (and, incidentally, f-score). These metrics give us a bit more insight into how our models do on each of our classes, with respect to different types of errors (e.g. false positives/negatives). We get these metrics in a nice table from [`scikit-learn`](http://scikit-learn.org)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to munge the dev data the same way we did the training data, for the LR, ConvNet, and FastTEXT use cases (remove @-mentions, turn into ndarrays, sequencify, *&c*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "dev_inputs, dev_labels = zip(*dev_data)\n",
    "\n",
    "X_dev = []\n",
    "for item in dev_inputs:\n",
    "    try:\n",
    "        X_dev.append(mention_patt.sub(\"AT_MENTION\", item.decode(\"utf8\")))\n",
    "    except Exception:\n",
    "        print item\n",
    "        break\n",
    "\n",
    "y_dev = np.asarray(dev_labels[:10000])\n",
    "y_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.801     0.790     0.796      5010\n",
      "          1      0.792     0.803     0.798      4990\n",
      "\n",
      "avg / total      0.797     0.797     0.797     10000\n",
      "\n",
      "CPU times: user 3.07 s, sys: 61.4 ms, total: 3.13 s\n",
      "Wall time: 3.13 s\n"
     ]
    }
   ],
   "source": [
    "%time print classification_report(y_dev[:10000], pipeline.predict(X_dev[:10000]), digits=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty decent numbers, and decently quick to classify 10k items.\n",
    "\n",
    "Let's see how the CNN model does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing\n",
      "Found 168 unique tokens\n",
      "Padding, encoding, train/dev split\n"
     ]
    }
   ],
   "source": [
    "dev_sequences, _ = tokenize_texts(X_dev[:10000])\n",
    "X_dev_cnn = pad_seqs(dev_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.785     0.711     0.746      5010\n",
      "          1      0.735     0.804     0.768      4990\n",
      "\n",
      "avg / total      0.760     0.757     0.757     10000\n",
      "\n",
      "CPU times: user 33.5 s, sys: 327 ms, total: 33.8 s\n",
      "Wall time: 8.5 s\n"
     ]
    }
   ],
   "source": [
    "%time print classification_report(y_dev[:10000], np.round(model.predict(X_dev_cnn[:10000])).astype(\"int32\"), digits=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm...four points lower on f-score, and almost 3 times slower in prediction. We'll need something better than this to take to the boss if we want to convince him to let us play with DL stuff.\n",
    "\n",
    "Let's see how FastText does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with codecs.open(\"data/sentiment.dev.ft.txt\", \"w\") as f_out:\n",
    "    for label, datum in zip(dev_labels, X_dev):\n",
    "        try:\n",
    "            f_out.write(\"%s %s\" % (\"__label__good\" if label else \"__label__bad\", datum.encode(\"utf8\")) + \"\\n\")\n",
    "        except UnicodeEncodeError:\n",
    "            print datum\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__bad AT_MENTION already worked out today \r\n",
      "__label__bad AT_MENTION Ah yes right. I don't know why didn't I think of it  Thanks a bunch!\r\n",
      "__label__good AT_MENTION tomorrow I should have some cools pics for you if weather is good \r\n",
      "__label__good AT_MENTION ADD me to your posse  NZKZKL #epicpetwars\r\n",
      "__label__bad AT_MENTION AT_MENTION thank you for the shows,thank you very much for everything and for all time here in Brazil. It was amazing.Good bye \r\n"
     ]
    }
   ],
   "source": [
    "!head -5 data/sentiment.dev.ft.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        bad      0.776     0.788     0.782      4907\n",
      "       good      0.793     0.781     0.787      5093\n",
      "\n",
      "avg / total      0.785     0.784     0.785     10000\n",
      "\n",
      "CPU times: user 510 ms, sys: 1.2 ms, total: 512 ms\n",
      "Wall time: 512 ms\n"
     ]
    }
   ],
   "source": [
    "y_ft = [u\"good\" if x else u\"bad\" for x in dev_labels]\n",
    "%time print classification_report(y_ft[:10000], np.array(ft.predict(X_dev[:10000])), digits=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expect, it's FAST; 6x faster than LR in prediction. Moreover, those numbers are pretty good; better than the ConvNet out of the box, and very nearly as good as the LR+BoW model (OK, it would have been cool if they were better).\n",
    "\n",
    "Let's see what we can do to beef our metrics up a bit for the ConvNet and FastText models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squeezing better performance out of our models\n",
    "### (wherein we try to show that CNNs and FastText are worth pursuing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by trying to retrain our ConvNet for twice as long.If that doesn't help, we can always try with more data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "100000/100000 [==============================] - 273s - loss: 0.4720 - acc: 0.7756   \n",
      "Epoch 2/4\n",
      "100000/100000 [==============================] - 276s - loss: 0.4535 - acc: 0.7853   \n",
      "Epoch 3/4\n",
      "100000/100000 [==============================] - 277s - loss: 0.4414 - acc: 0.7943   \n",
      "Epoch 4/4\n",
      "100000/100000 [==============================] - 279s - loss: 0.4288 - acc: 0.8011   \n",
      "CPU times: user 1h 11min 44s, sys: 1min 30s, total: 1h 13min 14s\n",
      "Wall time: 18min 26s\n"
     ]
    }
   ],
   "source": [
    "%time hist = model.fit(data[:100000], labels[:100000], batch_size=BATCH_SIZE, epochs=2*EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, we're basically at the same training time as for the LR pipeline now. The loss kept dropping, which is good, **BUT** do note that I'm checking against validation loss here; `model.fit()` lets you pass in a validation set as an additional param, and you can set up checks against the loss on the validation set---which doesn't get used as training data---to decide when to stop training. Normally I'd have done that.\n",
    "\n",
    "Alright, let's see how good this baby is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.719     0.864     0.785      5010\n",
      "          1      0.829     0.661     0.736      4990\n",
      "\n",
      "avg / total      0.774     0.763     0.760     10000\n",
      "\n",
      "CPU times: user 35.7 s, sys: 1 s, total: 36.7 s\n",
      "Wall time: 9.44 s\n"
     ]
    }
   ],
   "source": [
    "%time print classification_report(y_dev[:10000], np.round(model.predict(X_dev_cnn[:10000])).astype(\"int32\"), digits=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, not promising. We squeezed a tiny bit more performance out, in terms of average metrics, but we're still a few points away from the scores the basic system got.\n",
    "\n",
    "What happens when we train the FastText model for twice as long?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.4 s, sys: 2.46 s, total: 38.9 s\n",
      "Wall time: 18.3 s\n"
     ]
    }
   ],
   "source": [
    "%time ft.supervised(input=\"data/sentiment.train.ft.txt\", output=\"sentiment_model\", verbose=3, epoch=(2*EPOCHS), lr=0.1, minn=3, maxn=6, wordNgrams=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        bad      0.779     0.797     0.788      4907\n",
      "       good      0.800     0.783     0.791      5093\n",
      "\n",
      "avg / total      0.790     0.790     0.790     10000\n",
      "\n",
      "CPU times: user 524 ms, sys: 29.9 ms, total: 554 ms\n",
      "Wall time: 534 ms\n"
     ]
    }
   ],
   "source": [
    "%time print classification_report(y_ft[:10000], np.array(ft.predict(X_dev[:10000])), digits=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright...a teeny bit better. Can we do more? Well, one thing we did here to make things \"fair\" was train everything under the same regime, namely using on `{3,4,5,6}`-grams.\n",
    "From what I understand, FastText by default includes both word unigrams and character ngrams in its models. Let's let it do that (of course, to make this apples to apples, we'll want to retrain the LR+BoW model to allow this, as well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.5 s, sys: 2.35 s, total: 40.9 s\n",
      "Wall time: 17.6 s\n"
     ]
    }
   ],
   "source": [
    "%time ft.supervised(input=\"data/sentiment.train.ft.txt\", output=\"sentiment_model\", verbose=3, epoch=(2*EPOCHS), lr=0.1, minn=3, maxn=6, wordNgrams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        bad      0.781     0.807     0.794      4907\n",
      "       good      0.808     0.782     0.795      5093\n",
      "\n",
      "avg / total      0.795     0.794     0.794     10000\n",
      "\n",
      "CPU times: user 558 ms, sys: 30.7 ms, total: 588 ms\n",
      "Wall time: 568 ms\n"
     ]
    }
   ],
   "source": [
    "%time print classification_report(y_ft[:10000], np.array(ft.predict(X_dev[:10000])), digits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try training the ConvNet on twice as much data...don't forget that we truncated `data` right when we defined it up above, so let's revisit that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing\n",
      "Found 196 unique tokens\n",
      "Padding, encoding, train/dev split\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(500000, 256)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences, word_index = tokenize_texts(X_train[:500000])\n",
    "data = pad_seqs(sequences)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      " 13312/200000 [>.............................] - ETA: 507s - loss: 0.4519 - acc: 0.7866"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 189 is out of bounds for size 186\nApply node that caused the error: AdvancedSubtensor1(embedding_2/embeddings, Reshape{1}.0)\nToposort index: 48\nInputs types: [TensorType(float32, matrix), TensorType(int32, vector)]\nInputs shapes: [(186, 32), (16384,)]\nInputs strides: [(128, 4), (4,)]\nInputs values: ['not shown', 'not shown']\nOutputs clients: [[Reshape{3}(AdvancedSubtensor1.0, MakeVector{dtype='int64'}.0)]]\n\nBacktrace when the node is created(use Theano flag traceback.limit=N to make it longer):\n  File \"/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-114-3a8528a90ed6>\", line 6, in <module>\n    trainable=True)(x)\n  File \"/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/keras/engine/topology.py\", line 602, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/keras/layers/embeddings.py\", line 134, in call\n    out = K.gather(self.embeddings, inputs)\n  File \"/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/keras/backend/theano_backend.py\", line 483, in gather\n    y = reference[indices]\n\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-161-d40484337dfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time hist = model.fit(data[:200000], labels[:200000], batch_size=BATCH_SIZE, epochs=2*EPOCHS)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2157\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2158\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2077\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2078\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2079\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2080\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m             \u001b[0;32mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    896\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[1;32m    899\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m                 \u001b[0;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/theano/gof/link.pyc\u001b[0m in \u001b[0;36mraise_with_op\u001b[0;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# extra long error message in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 189 is out of bounds for size 186\nApply node that caused the error: AdvancedSubtensor1(embedding_2/embeddings, Reshape{1}.0)\nToposort index: 48\nInputs types: [TensorType(float32, matrix), TensorType(int32, vector)]\nInputs shapes: [(186, 32), (16384,)]\nInputs strides: [(128, 4), (4,)]\nInputs values: ['not shown', 'not shown']\nOutputs clients: [[Reshape{3}(AdvancedSubtensor1.0, MakeVector{dtype='int64'}.0)]]\n\nBacktrace when the node is created(use Theano flag traceback.limit=N to make it longer):\n  File \"/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-114-3a8528a90ed6>\", line 6, in <module>\n    trainable=True)(x)\n  File \"/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/keras/engine/topology.py\", line 602, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/keras/layers/embeddings.py\", line 134, in call\n    out = K.gather(self.embeddings, inputs)\n  File \"/Users/fredmailhot/anaconda/envs/fast-ai/lib/python2.7/site-packages/keras/backend/theano_backend.py\", line 483, in gather\n    y = reference[indices]\n\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "%time hist = model.fit(data[:200000], labels[:200000], batch_size=BATCH_SIZE, epochs=2*EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
